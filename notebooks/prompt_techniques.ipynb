{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3284303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\viraj\\Zuu\\AI_Engineer_Bootcamp\\notebooks\n",
      "Parent directory exists: True\n",
      "Utils directory exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "# Verify the path is correct\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Parent directory exists: {os.path.exists('..')}\")\n",
    "print(f\"Utils directory exists: {os.path.exists('../utils')}\")\n",
    "\n",
    "from utils.prompts import render\n",
    "from utils.llm_client import LLMClient #pick llm client\n",
    "from utils.logging_utils import log_llm_call #log every api call\n",
    "from utils.router import  pick_model, should_use_reasoning_model #general, reasoning, strrong\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9988b180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\viraj\\\\Zuu\\\\AI_Engineer_Bootcamp\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f673d30",
   "metadata": {},
   "source": [
    "### 01- Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f07414f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the sentiment is : neutral',\n",
       " 'usage': {'input_tokens_est': 101,\n",
       "  'context_tokens_est': 0,\n",
       "  'total_est': 104,\n",
       "  'prompt_tokens_actual': 130,\n",
       "  'completion_tokens_actual': 6,\n",
       "  'total_tokens_actual': 136},\n",
       " 'latency_ms': 420,\n",
       " 'raw': ChatCompletion(id='chatcmpl-179264fe-847e-4119-9c71-939bc80143a9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='the sentiment is : neutral', role='assistant', annotations=None, executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1769920195, model='llama-3.1-8b-instant', object='chat.completion', mcp_list_tools=None, service_tier='on_demand', system_fingerprint='fp_4387d3edbb', usage=CompletionUsage(completion_tokens=6, prompt_tokens=130, total_tokens=136, completion_time=0.007807293, completion_tokens_details=None, prompt_time=0.007174466, prompt_tokens_details=None, queue_time=0.050534224, total_time=0.014981759), usage_breakdown=None, x_groq=XGroq(id='req_01kgbqdtcyf31bwts522qvpzge', debug=None, seed=1058308425, usage=None)),\n",
       " 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text , spec = render(\n",
    "    \"zero_shot.v1\",\n",
    "    role = \"sentiment_analyst\",\n",
    "    instruction = \"Analyze the following text and determine the sentiment as positive/negative/neutral\",\n",
    "    constraints = \"the sentiment should be one of the following: positive, negative, neutral\",\n",
    "    format = \"the sentiment is : {sentiment}\"\n",
    "    )\n",
    "\n",
    "\n",
    "model = pick_model(\"groq\", \"general\")\n",
    "llm = LLMClient(\"groq\", model)\n",
    "\n",
    "text = \"It seems nice but i dont like it \"\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You must respond in EXACTLY this format with no additional text: 'the sentiment is : [positive/negative/neutral]'\"},\n",
    "        {\"role\": \"user\", \n",
    "        \"content\": f\"{prompt_text} Review: {text}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "llm.chat(messages, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38b3cb",
   "metadata": {},
   "source": [
    "### 02_Few Shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c4083",
   "metadata": {},
   "source": [
    "provide an exmple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4468c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the sentiment is : negative',\n",
       " 'usage': {'input_tokens_est': 236,\n",
       "  'context_tokens_est': 0,\n",
       "  'total_est': 239,\n",
       "  'prompt_tokens_actual': 265,\n",
       "  'completion_tokens_actual': 6,\n",
       "  'total_tokens_actual': 271},\n",
       " 'latency_ms': 505,\n",
       " 'raw': ChatCompletion(id='chatcmpl-ab6f42e8-d4ca-4517-8853-411c8ca84e9f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='the sentiment is : negative', role='assistant', annotations=None, executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1769920183, model='llama-3.1-8b-instant', object='chat.completion', mcp_list_tools=None, service_tier='on_demand', system_fingerprint='fp_1151d4f23c', usage=CompletionUsage(completion_tokens=6, prompt_tokens=265, total_tokens=271, completion_time=0.008887368, completion_tokens_details=None, prompt_time=0.016455775, prompt_tokens_details=None, queue_time=0.054586458, total_time=0.025343143), usage_breakdown=None, x_groq=XGroq(id='req_01kgbqdfcxedysjtm2rt5en2xv', debug=None, seed=1140432611, usage=None)),\n",
       " 'meta': {'retry_count': 0, 'backoff_ms_total': 0, 'overflow_handled': False}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = \"\"\"\n",
    "example 01:\n",
    "review: \"I absolutely love this product! It has changed my life for the better.\"\n",
    "sentiment: positive\n",
    "\n",
    "example 02:\n",
    "review: \"This is the worst purchase I've ever made. Completely disappointed.\"\n",
    "sentiment: negative\n",
    "\n",
    "example 03:\n",
    "review: \"The product is okay, nothing special but not terrible either.\"\n",
    "sentiment: neutral\n",
    "\n",
    "example 04:\n",
    "review: \"im really happy with the product its bad\"  \n",
    "sentiment: negative\n",
    "\n",
    "example 05:\n",
    "review: \"The service was excellent and the staff were very friendly.\"\n",
    "sentiment: positive\n",
    "\n",
    "example 06:\n",
    "review: \"im really unhappy with the product but its amazing\"  \n",
    "sentiment: positive\n",
    "\"\"\"\n",
    "\n",
    "prompt_text , spec = render(\n",
    "    \"few_shot.v1\",\n",
    "    role = \"sentiment_analyst\",\n",
    "    examples = examples,\n",
    "    query = \"\",\n",
    "    constraints = \"the sentiment should be one of the following: positive, negative, neutral\",\n",
    "    format = \"the sentiment is : {sentiment}\"\n",
    "    )\n",
    "\n",
    "\n",
    "model = pick_model(\"groq\", \"general\")\n",
    "llm = LLMClient(\"groq\", model)\n",
    "\n",
    "text = \"It seems nice but i dont like it\"\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You must respond in EXACTLY this format with no additional text: 'the sentiment is : [positive/negative/neutral]'\"},\n",
    "        {\"role\": \"user\", \n",
    "        \"content\": f\"{prompt_text} Review: {text}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "llm.chat(messages, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83dfdf4",
   "metadata": {},
   "source": [
    "### 03_COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ce7015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reasoning model: llama-3.3-70b-versatile\n",
      "CoT Response (Travel Time with Break):\n",
      "================================================================================\n",
      "## Step 1: Identify if the car stopped during the journey and the total travel time.\n",
      "The car stopped for 40 minutes during the 2-hour journey.\n",
      "\n",
      "## Step 2: Convert the stopping time into hours to calculate the actual driving time.\n",
      "40 minutes is equal to 40/60 = 2/3 hours.\n",
      "\n",
      "## Step 3: Calculate the actual driving time by subtracting the stopping time from the total travel time.\n",
      "Actual driving time = 2 hours - 2/3 hours = 2 - 2/3 = (6/3) - (2/3) = 4/3 hours.\n",
      "\n",
      "## Step 4: Calculate the average speed for the entire journey.\n",
      "Average speed for the entire journey = Total distance / Total time = 100 miles / 2 hours = 50 miles per hour.\n",
      "\n",
      "## Step 5: Calculate the average speed during the actual driving time.\n",
      "Average speed during driving = Total distance / Actual driving time = 100 miles / (4/3 hours) = 100 * (3/4) = 75 miles per hour.\n",
      "\n",
      "## Step 6: Answer the questions based on the calculations.\n",
      "Question 1: The average speed of the car for the entire journey is 50 miles per hour.\n",
      "Question 2: The average speed during the actual driving time is 75 miles per hour.\n",
      "\n",
      "Answer:\n",
      "1. 50 mph\n",
      "2. 75 mph\n",
      "================================================================================\n",
      "\n",
      "Usage: {'input_tokens_est': 246, 'context_tokens_est': 0, 'total_est': 249, 'prompt_tokens_actual': 278, 'completion_tokens_actual': 290, 'total_tokens_actual': 568}\n",
      "Latency: 919ms\n"
     ]
    }
   ],
   "source": [
    "# CoT auto-routes to reasoning model\n",
    "reasoning_model = pick_model('groq', 'cot')\n",
    "print(f'Using reasoning model: {reasoning_model}')\n",
    "\n",
    "client_reasoning = LLMClient('groq', reasoning_model)\n",
    "\n",
    "# Problem from live class: break time vs travel time confusion\n",
    "problem = \"\"\"A car travels 100 miles in 2 hours. \n",
    "\n",
    "Question 1: What is the average speed of the car?\n",
    "Question 2: If the car stopped for 40 minutes during this 2-hour journey, \n",
    "what was the average speed during the actual driving time?\n",
    "\n",
    "Important: The 2 hours already includes the 40-minute stop.\"\"\"\n",
    "\n",
    "# Additional guidance\n",
    "instruction = \"\"\"Solve the following problem step by step.\n",
    "1. First identify whether the car travelled the entire time without stopping or not.\n",
    "2. If car stopped for x minutes and overall travelled for y hours, the actual driving duration is y-x.\n",
    "3. If stopping time x is mentioned, do not add it to the travel duration because it's already included.\n",
    "   So actual travel time is y (total time) - x (stopping time).\n",
    "4. If car travelled the entire time without stopping, then average speed is distance / y.\n",
    "5. If car stopped for x minutes, then average speed during driving is distance / (y - x).\"\"\"\n",
    "\n",
    "prompt_text, spec = render(\n",
    "    'cot_reasoning.v1',\n",
    "    role='math tutor',\n",
    "    problem=problem\n",
    ")\n",
    "\n",
    "# Combine problem with instruction \n",
    "full_prompt = f\"\"\"text: {prompt_text}\n",
    "\n",
    "instruction: {instruction}\"\"\"\n",
    "\n",
    "messages = [{'role': 'user', 'content': full_prompt}]\n",
    "response = client_reasoning.chat(messages, temperature=spec.temperature, max_tokens=spec.max_tokens)\n",
    "\n",
    "print('CoT Response (Travel Time with Break):')\n",
    "print('=' * 80)\n",
    "print(response['text'])\n",
    "print('=' * 80)\n",
    "print(f\"\\nUsage: {response['usage']}\")\n",
    "print(f\"Latency: {response['latency_ms']}ms\")\n",
    "log_llm_call('groq', reasoning_model, 'cot', response['latency_ms'], response['usage'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-engineering-essentials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
